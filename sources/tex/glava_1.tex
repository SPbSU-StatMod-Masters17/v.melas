

\chapter{Нелинейный регрессионный анализ}



В настоящей главе мы изложим некоторые основные результаты,
относящиеся к изучению нелинейных по параметрам моделей общего вида.


\section{\bf  Нелинейная (по параметрам) регрессионная модель}

Пусть $N$ --- заданное натуральное число. Пусть результаты
эксперимента $y_1,\ldots,y_N\in {\IR}^1$ описываются уравнением \beq
y_i=\eta(x_j,\Theta)+\varepsilon_j, \eeq где $x_1,\ldots,x_N\in
{\mbox{\rm{\Large{\eufrac X}}}}$, {\Large{\eufrac  X}} --- некоторое
множество, относительно которого мы сделаем некоторые предположения
в дальнейшем,

$\eta(x,\Theta)$ --- функция, известная с точностью до вектора
параметров $\Theta=(\theta_1,\ldots,\theta_m)^T$,

$\varepsilon_j$ --- случайные ошибки --- некоррелированные случайные
величины такие, что \beq E\varepsilon_j=0,\quad {\cal
D}\varepsilon_j=\sigma^2. \eeq

Цель эксперимента --- оценка вектора параметров $\Theta$. Будем
говорить, что параметр $\theta_j$, $j\in1:m$ входит в модель (1.1)
нелинейным образом, если при фиксированном $x$ функция
$$
\frac{\partial\eta(x,\Theta)}{\partial\theta_j}
$$
 существует и не является постоянной.
Если эта функция является постоянной, то будем говорить, что
соответствующий параметр входит в модель линейным образом. Функцию
регрессии $\eta(x,\Theta)$ будем называть нелинейной (по
параметрам), если хотя бы один параметр $\theta_j$ входит в модель
(1.1) нелинейным образом.

Необходимо выбрать экспериментальные условия $x_1,\ldots,x_N$ и
метод оценивания вектора параметров.

Введем два понятия плана эксперимента.

Под {\it точным планом эксперимента} мы будем понимать набор $N$
точек, не обязательно различных, из заданного множества {\eufrac X}.
Под {\it приближенным планом эксперимента} будем понимать дискретную
вероятностную меру, задаваемую таблицей
$$
\xi=\{x_1,\ldots,x_n;\mu_1,\ldots,\mu_n\},
$$
 где $x_i\ne x_j$ $(i\ne j)$, $x_i\in{\mbox{\eufrac X}}$, $\mu_i> 0$, $i,j=1,\ldots,n$,
$\sum^n_{i=1}\mu_i=1$, $n$ --- произвольное натуральное число.

Понятие приближенного плана эксперимента было введено Дж.\, Кифером.
Оно существенно облегчает математические исследования. При
практическом использовании приближенных планов рекомендуют при
условиях $x_j$ проводить приблизительно $\mu_jN$ экспериментов,
$j=1,\ldots,n$.

Наилучший (в некотором точно определенном смысле) выбор плана ---
задача, которую мы рассмотрим в разделе 1.3.

При фиксированном (выбранном некоторым способом) плане в качестве
метода оценивания будем использовать (нелинейный) метод наименьших
квадратов (МНК), введенный Гауссом. По определению МНК-оценка есть
вектор $\hat\Theta$, являющийся решением экстремальной задачи
$$
\sum^N_{j=1}\left(\eta(x_j,\Theta)-y_j\right)^2\to
\min_{\Theta\in{\IR}^m}.
$$

МНК--оценки обладают замечательными асимптотическими свойствами,
описанными в следующем разделе.


\section{\bf Асимптотические свойства МНК-оценок}

Пусть $\Omega$ --- ограниченное замкнутое множество в ${\IR}^m$,
${\mbox{\eufrac X}}$ --- ограниченное замкнутое множество в
${\IR}^k$, где $k$ --- некоторое натуральное число.

Пусть функция регрессии $\eta(x,\Theta)$ нелинейна по параметрам и
определена при всех $x\in{\mbox{\eufrac X}}$, $\Theta\in\Omega$.
Через $\Theta_u$ будем обозначать истинное значение вектора
параметров, т.\,е. такое значение $\Theta$, при котором верна модель
(1.1).

Далее под планом будет понимать приближенный план, если не оговорено
противное. Для дискретных мер
$\xi=\{x_1,\ldots,x_n;\mu_1,\ldots,\mu_n\}$ будем использовать
запись
$$
\int_{\mbox{\eufrac X}} g(x)\xi(dx)=\sum^n_{i=1}g(x_i)\mu_i,
$$
где $g$ --- произвольная функция.

Введем следующие предположения: \bi
\item[a)] функция $\eta(x,\Theta)$ непрерывна на ${\mbox{\eufrac X}}\times\Omega$;
\item[б)] последовательность планов $\{\xi_N\}$ слабо сходится к плану $\xi$,
т.\,е. для любой непрерывной функции $g(x)$ на {\eufrac X} при
$N\to\infty$ имеет место соотношение
$$
\int_{\mbox{\eufrac X}} g(x)\xi_N(dx)\to \int_{\mbox{\eufrac X}}
g(x)\xi(dx);
$$
\item[в)] величина
$$
\int_{\mbox{\eufrac X}} [\eta(x,\Theta)-\eta(x,\bar\Theta)]^2\xi(dx)
$$
при $\bar\Theta$, $\Theta\subset\Omega$ равна нулю только при
$\Theta=\bar\Theta$;
\item[г)] производные
$$
\partial\eta/\partial\theta_i,\quad \partial^2\eta/\partial\theta_i
\partial\theta_j,\quad i,j=1,\ldots,m
$$
существуют и непрерывны на ${\mbox{\eufrac X}}\times\Omega$;
\item[д)] $\hat\Theta_u$ --- внутренняя точка $\Omega$ и матрица
$$
M(\xi,\Theta)=\int_{\mbox{\eufrac X}}
f(x,\Theta)f^T(x,\Theta)\xi(dx),
$$
где
$$
f^T(x,\Theta)=\left(\frac{\partial\eta(x,\Theta)}{\partial\theta_1},\ldots,
\frac{\partial\eta(x,\Theta)}{\partial\theta_m}\right)
$$
невырождена при $\Theta=\Theta_u$. \ei

Пусть $\xi_N$ имеет вид
$$
\{x_1,\ldots,x_N;\frac{1}{N},\ldots\frac{1}{N}\},
$$
где $x_i$ --- не обязательно различные точки, \beq
\hat\Theta_N={\rm{arg}}\,\min_{\Theta\in\Omega}\sum^N_{i=1}
(\eta(x_i,\Theta)-y_i)^2. \eeq

{\bf Теорема 1.2.1.} {\it Если случайные ошибки одинаково
распределены, результаты экспериментов описываются уравнением
(1.1)--(1.2) и выполнены предположения а)--в), то последовательность
МНК-оценок сильно состоятельна, т.\,е. при $N\to\infty$
$$
\hat\Theta_{(N)}\to\Theta_u
$$
с вероятностью 1, где $\hat\Theta_N$ определено формулой (1.3).
Если, кроме того, выполняются предположения г) и д), то при
$N\to\infty$ последовательность распределений случайных векторов
$\sqrt{N}(\hat\Theta_N-\Theta_u)$ сходится к нормальному
распределению с нулевым вектором средних и дисперсионной матрицей
$\sigma^2M^{-1}(\xi,\Theta_u)$.}

Доказательство этой теоремы можно найти в статье (Jennrich, 1969).

Матрица $M(\xi,\Theta_u)$ называется информационной матрицей (для
нелинейных по параметрам регрессионных моделей).
\bigskip

\section{\bf Локально оптимальные планы эксперимента}

Для линейных по параметрам регрессионных моделей наиболее
распространенным критерием оптимальности плана эксперимента является
$D$-критерий. Согласно этому критерию оптимальными считаются планы,
максимизирующие определитель информационной матрицы. В
статистическом смысле это означает, что МНК-оценки, построенные по
результатам экспериментов в соответствии с $D$-оптимальным планом,
будут иметь наименьший объем доверительного эллипсоида. Кроме того,
согласно теореме эквивалентности Кифера--Вольфовица, максимальная по
$x\in{\mbox{\eufrac X}}$ дисперсия оценки значения функции в точке
$x$, также будет минимальной. Подробнее об этом см. (Мелас, 1999,
Введение; Ермаков, Жиглявский, 1987, гл.\,2).

В случае нелинейных по параметрам моделей информационная матрица,
как видно из предыдущего раздела, зависит от истинного значения
вектора параметров, которое, разумеется, неизвестно исследователю.
Предположим, однако, что известно некоторое приближение
$\Theta^{(0)}$. Планы, максимизирующие определитель матрицы
$M(\xi,\Theta)$ при $\Theta=\Theta^{(0)}$ будем называть {\it
локально $D$-оптимальными}.

Эти планы зависят, вообще говоря, от вектора $\Theta^{(0)}$, хотя в
некоторых случаях могут и не зависеть от него.

Линеаризируем модель в окрестности точки $\Theta=\Theta^{(0)}$:
$$
\eta(x,\Theta)-\eta(x,\Theta^{(0)})\approx(\Theta-\Theta^{(0)})^T
f(x,\Theta^{(0)}).
$$

Для линейной модели, представленной в правой части этого
приближенного равенства, справедлива следующая теорема, которая
будет служить одним из инструментов исследования.

Введем обозначения: \bea f^T(x)&=& f^T(x,\Theta^{(0)})=\left(
\frac{\partial\eta(x,\Theta^{(0)})}{\partial\theta_1},
\ldots,\frac{\partial\eta(x,\Theta^{(0)})}{\partial\theta_m}\right),
\nonumber \\
M(\xi)&=& M(\xi,\Theta^{(0)})=\int_{\mbox{\eufrac X}} f(x)
f^T(x)\xi(dx) = \sum^n_{i=1} f(x_i)
f^T(x_i)\mu_i, \nonumber \\
d(x,\xi)&=& f^T(x) M^{-1}(\xi)f(x).\nonumber \eea

{\bf Теорема 1.3.1.} {\it План $\xi^*$ является локально
$D$-оптимальным для модели (1.1)-(1.2) тогда и только тогда, когда
$$
\max_{x\in{\mbox{\eufrac X}}} d(x,\xi^*)=m.
$$
Кроме того,
$$
\max_{x\in{\mbox{\eufrac X}}} d(x,\xi^*)=\inf_\xi
\max_{x\in{\mbox{\eufrac X}}} d(x,\xi),
$$
где нижняя грань берется по всем (приближенным) планам эксперимента,
и функция $d(x,\xi^*)$ достигает своего максимального значения во
всех точках любого локально $D$-оптимального плана. Информационные
матрицы всех локально $D$-оп\-ти\-маль\-ных планов совпадают.}

Теорема 1.3.1 является простой переформулировкой теоремы
эквивалентности Кифера--Вольфовица, доказательство которой можно
найти в книге (Ермаков, Жиглявский, 1987, с.\,109).

Заметим, что при $n=m$ информационная матрица $M(\xi)$ принимает вид
$$
M(\xi)=X^T\Lambda X,
$$
где $\Lambda = {\rm{diag}}\,\{\mu_1,\ldots,\mu_m\}$,
$X=(f_i(x_j))^m_{j,i=1}$, что легко проверить перемножением матриц в
правой части. Поэтому
$$
\det M(\xi)=(\det X)^2\mu_1\ldots\mu_m.
$$
Заметим, что $\prod^m_{i=1}\mu_i
\leq\left(\sum^m_{i=1}\mu_i/m\right)^m$ в силу неравенства между
средним арифметическим и средним геометрическим неотрицательных
чисел. Поэтому для планов с числом точек, равным $m$, оптимальный
выбор весов имеет вид $\mu_i=1/m$, $i=1,\ldots,m$. Для таких планов
достаточно изучить задачу
$$
(\det X)^2\to\max_{x_1,\ldots,x_m\in {\mbox{\eufrac X}}}.
$$

Точные планы вида $\zeta=\{x_1,\ldots,x_m\}$ будем называть {\it
насыщенными}. Так как $\det X=0$ если хотя бы две точки плана
совпадают, то можно ограничиться исследованием насыщенных планов,
для которых $x_i\ne x_j$\, $(i\ne j)$.

Если число точек в любом локально $D$-оптимальном плане равно числу
параметров (а это имеет место для дробно-рациональной функции
регрессии, как будет показано в главе 3), то можно ограничиться
изучением насыщенных планов. Локально $D$-оптимальный (приближенный)
план при этом можно получить
 добавлением равных весов к точкам
насыщенного локально $D$-оптимального плана, т.\,е. плана локально
$D$-оптимального в классе всех насыщенных планов.

В следующей главе излагается подход, позволяющий изучить зависимость
точек и весов локально оптимальных планов от вектора $\Theta^{(0)}$.
