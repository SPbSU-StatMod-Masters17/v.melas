\section{Асимптотические свойства нелинейного метода наименьших квадратов}

\textit{Изложение материала данного вопроса имеется в разделе 1.2 Учебного Пособия: <<Локально Оптимлаьные Планы Эксперимента>>.
        Для данного вопроса необходимо понимать, как устроена нелинейная регрессионная модель (вопрос 2).}

\paragraph{Устройство нелинейной модели и основные понятия.}
Заданы $N \in \N$ (объем выборки), $m \in \N$, $\Theta \in \R^m$ (неизвестный многомерный параметр), $\mathcal X$ --- некоторое множество\footnote{
    В самом общем описании, никакие условия на это множество не накладываются.
}.
Пусть происходит <<эксперимент>>, в котором наблюдаются (одномерные) <<результаты эксперимента>> $y_1, y_2, \ldots, y_N \in \R^1$.
Рассмотрим отображение $\eta \colon \mathcal X \times \R^m \mapsto \R^1$.
Аналитическое задание отображения $\eta$ как функции двух аргументов нам известно.

Модель эксперимента задается следующим образом: для всех $j \in 1:N$
\begin{gather}
    \label{sec1::eq::nonlinear_model}
    y_j = \eta(x_j, \Theta) + \eps_j,
\end{gather}
где $x_1, x_2, \ldots, x_N \in \mathcal X$ --- <<условия эксперимента>>, $\Theta = \Tr{(\theta_1, \theta_2, \ldots, \theta_m)} \in \Omega \subset \R^m$,
а $\eps_1, \eps_2, \ldots, \eps_N$ --- некоррелированные, центрированные, гомоскедастичные случайные величины, т.е.
$\mathbb E \eps_j = 0$ и $\mathbb D \eps_j = \sigma^2$ для всех $j \in 1:N$.\footnote{Естественно $\eps_j$ играют роль ошибок измерения, шума.}

\textbf{Задача:} оценить параметр $\Theta$.
Ясно, что задача является регрессионной, причем функция $\eta$ является регрессией.\footnote{Мы опускаем здесь вопросы аккуратной формализации
задачи, так чтобы полностью оправдать термин регрессии, как условного математического ожидания.}

Нужно формально объяснить, что значит <<нелинейная модель>>, то есть чем эта модель отличается от <<линейной>>.
Будем говорить, что параметр $\theta_j$, где $j \in 1:m$, входит нелинейно в модель \eqref{sec1::eq::nonlinear_model},
если при фиксированном $x$
\begin{gather*}
    \frac{\partial \eta(x, \cdot)}{\partial \theta_j}
\end{gather*}
существует и не является постоянной. Если же указанная функция является постоянной, то говорим, что параметр $\theta_j$ входит в модель линейно.
Если есть хотя бы один параметр $\theta_j$, который входит в модель нелинейно, то модель \eqref{sec1::eq::nonlinear_model} называют нелинейной.
Регрессию $\eta$ в таком случае тоже называют нелинейной (по параметрам).

Для того, чтобы определить неизвестный многомерный параметр $\Theta$, нужно выбрать эксперементальные условия $x_1, x_2, \ldots, x_N$ и метод
оценивания параметров. Определимся сначала с первым вопросом.
\begin{dfn}
    Любой набор из (не обязательно различных) $N$ элементов множества $\mathcal X$ будем называть точным планом эксперимента.
\end{dfn}
\begin{dfn}
    Пусть $n$ --- фиксированное натуральное число.
    Приближенным (непрерывным) планом эксперимента называют дискретную вероятностную\footnote{то есть нормированную на единицу.} меру, задаваемую таблицей
    \begin{gather}
        \xi = \{x_1, x_2, \ldots, x_n; \mu_1, \mu_2, \ldots, \mu_n\},
    \end{gather}
    где $x_j$ различные, $\mu_i > 0$ для всех $i$, а $\sum_{i=1}^n \mu_i = 1$\footnote{Подразумевается, что $\xi(x_i) = \mu_i$ для всех $i$}.
\end{dfn}
Заметим, что все условия, наложенные на меру являются простыми (необременняющими) и естественными.

Выбор <<наилучшего>> плана является отдельной задачей. Пусть план фиксирован, тогда в качестве метода оценивания параметров рассмотрим (нелинейный)
метод наименьших квадратов.
Будем обозначать $\hat \Theta$ --- решение экстремальной задачи МНК:
\begin{gather*}
    \sum_{j = 1}^N (\eta(x_j, \Theta) - y_j)^2 \to \min_{\Theta \in \R^m}.
\end{gather*}

Оценки $\hat \Theta$ обладают хорошими асимптотическими свойствами.

\paragraph{Асимптотические свойства МНК-оценок.}
В данном разделе мы начинаем вводить ограничения на множества $\Omega$ и $\mathcal X$.
Пусть $\Omega$ --- ограниченное замкнутое множество в $\R^m$, $\mathcal X$ --- ограниченное замкнутое множество в $\R^k$,
где $k \in \N$.

Пусть регрессия $\eta(x, \Theta)$ нелинейна по параметрам и определена при всех $x \in \mathcal X$, $\Theta \in \Omega$.
Через $\Theta_u$ будем обозначать истинное значение вектора параметров, т.е. такое значение $\Theta$, при котором верна модель
\eqref{sec1::eq::nonlinear_model}.

Под планом в дальнейшем всегда подразумеваем приближенный. Для дискретных мер $\xi = \{x_1, x_2, \ldots, x_n; \mu_1, \mu_2, \ldots, \mu_n\}$
используем стандартную запись (интеграл по мере, 2 курс):
\begin{gather*}
\int_\mathcal X g(x)\, d\xi(x) = \sum_{i = 1}^n g(x_i) \mu_i,
\end{gather*}
где $g$ --- произвольная функция, определенная на $\mathcal X$\footnote{На самом деле тут должна быть измеримость по мере, почему мы ее не требуем? Подумайте!}.

Пусть 
\begin{gather*}
    \xi_N = \{x_1, x_2, \ldots, x_N; 1/N, 1/N, \ldots 1/N\},
\end{gather*}
где $x_i$ --- необязательно различные точки,

{\color{blue} У нас здесь неявно предполагается, что на самом деле есть распределение на множестве точек плана видимо. Иначе непонятно, как $N \to \infty$... }

Введем предположения:
\begin{enumerate}
    \item
    регрессия $\eta(x, \Theta)$ непрерывна на множестве $\mathcal X \times \Omega$;

    \item 
    имеется слабая сходимость распределений $\mathcal L(\xi_N) \Rightarrow \mathcal L(\xi)$, где $\xi$ --- некоторый план, то есть
    для любой функции $g \in \mathrm C(\mathcal X)$ имеет место сходимость
    \begin{gather*}
        \int_\mathcal X g(x)\, d \xi_N(x) \rightarrow  \int_\mathcal X g(x)\, d \xi(x);
    \end{gather*}

    \item 
    для $\Theta, \overline \Theta \in \Omega$ величина
    \begin{gather*}
        \int_\mathcal X \left(\eta(x, \Theta) - \eta(x, \overline \Theta)\right)^2\, d\xi(x)
    \end{gather*}
    равна нулю только при $\Theta = \overline \Theta$\footnote{тогда и только тогда, правда?};

    \item
    Частные производные первого и второго порядка регрессии $\eta$ по параметру существуют и непрерывны на $\mathcal X \times \Omega$,
    то есть $\eta \in \mathrm C_\Theta^2(\mathcal X \times \Omega)$.

    \item
    Истинное значение параметра $\Theta_u$ является внутренней точкой $\Omega$\footnote{то есть не принадлежит $\mathrm {frac}(\Omega)$.
    Это существенно, так как множество $\Omega$ является замкнутым.}.

    \item
    Матрица\footnote{Убедитесь, что понимаете, что это, действительно, матрица.}
    \begin{gather}
        M(\xi, \Theta) = \int_\mathcal X f(x, \Theta) \Tr{f}(x, \Theta)\, d\xi(x),
    \end{gather}
    где
    \begin{gather*}
        f(x, \Theta) = \Tr{\left(\frac{\partial \eta(x, \Theta)}{\partial \theta_1}, \frac{\partial \eta(x, \Theta)}{\partial \theta_2}, \ldots,
        \frac{\partial \eta(x, \Theta)}{\partial \theta_m}\right)}
    \end{gather*}
    не вырождена при $\Theta = \Theta_u$.
\end{enumerate}

Обозначим
\begin{gather}
    \label{sec1::eq::ls_est}
    \hat \Theta_N = \argmin_{\Theta \in \Omega} \sum_{i = 1}^N (\eta(x_i, \Theta) - y_i)^2.
\end{gather}

\begin{thm}[без доказательства]
    Если случайные ошибки $\{\eps_i\}_{i=1}^N$ некоррелированы, одинаково распределены и являются центрированными и гомоскедастичными, результаты
    экспериментов описываются уравнением \eqref{sec1::eq::nonlinear_model} и выполнены предположения 1--3,
    то последовательность МНК-оценок сильно состоятельна,
    т. е. при $N \to \infty$
    \begin{gather*}
        \hat \Theta_N \to \Theta_u
    \end{gather*}
    с вероятностью 1, где $\hat \Theta_N$ определено формулой \eqref{sec1::eq::ls_est}.

    Если дополнительно выполняются предположения 4--6, то последовательность случайных векторов $\sqrt N (\hat \Theta_N - \Theta_u)$
    имеет асимптотически нормальное распределение с нулевым вектором средних и ковариационной матрицей $\sigma^2 M^{-1}(\xi, \Theta_u)$.
    \footnote{Вспомните, откуда тут $\sigma$.}
\end{thm}

Матрицу $M(\xi, \Theta_u)$ называют информационной матрицей для нелинейных по параметрам регрессионных моделей.
