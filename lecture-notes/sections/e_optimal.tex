\section{E-оптимальные планы}
\subsection{Определение и статистический смысл}
Пусть $M(\xi)$ — информационная матрица плана. 
\begin{dfn}
Будем говорить, что план $\xi$ является $E$-оптимальным, если 
$$ \xi = \argmin\limits_{\lambda_{\min}(M(\xi))}, \text{ где}$$
$\lambda_{\min}(M(\xi))$ — минимальное собственное число $M(\xi)$. 
\end{dfn}
Статистический смысл этого критерия состоит в минимизации дисперсии следующего выражения:
$$D(\Inner{p}{\theta}), \text{ где } p \in \R^m, ||p||_2 = 1$$
$$D(\Inner{p}{\theta}) = \Tr{p}M^{-1}(\xi)p$$
Из последней формулы видно\footnote{вспоминаем линейную алгебру и то, что $M^{-1}$ является положительно-определенной матрицой}, что максимум этого выражения достигается на первом собственном векторе, а cам максимум равен первому собственному числу. Собственные числа $\frac{1}{\lambda_i}$ матрицы $M^{-1}$ совпадают с обратными к  $\lambda_i$ — собственным числам матрицы $M$. Из такого представления следует, что $E$-оптимальность означает минимизацию максимальной длины оси доверительного эллипсоида для МНК-оценки\footnote{Эта ось, как все помнят, совпадает с направлением первого собственного вектора матрицы $D(\theta)$.}.


{\color{blue} что сюда еще надо?}
\subsection{Теорема эквивалентности}
\begin{dfn}
Обозначим класс неотрицательно-определенных симметричных матриц с единичным следом за $\mathbb{A}$. 
$$\mathbb{A} = \{ A | A \text{ p.s.d.}, \trace{A} = 1 \}$$
\end{dfn}
\begin{thm}
Пусть $f(x)=\Tr{(f_1(x), …, f_n(x)}$, $x \in \mathbb{X}$ является непрерывной функцией. Тогда:
\begin{enumerate}
\item План $\xi^{*}$ является $E$-оптимальным тогда и только тогда, когда 
$$\exists A^{*} \in \mathbb{A} \max\limits_{x \in \mathbb{X}} \Tr{f(x)}A^{*}f(x) \leq \lambda_{\min}(M(\xi^*))$$
\item $$\Tr{f(x_i^*)}A^{*}f(x_i^*) = \lambda_{\min}(M(\xi^*)), \text{ где}$$
для $i=1…n$ $x_i^*$ являются опорными точками $E$-оптимального плана.
\item $$ \min\limits_{A}\max\limits_{x \in \mathbb{X}} \Tr{f(x)}Af(x) = \max\limits_{\xi}\lambda_{\min}M(\xi)$$
\end{enumerate}
\end{thm}


Эта теорема является следствием общей теоремы о минимаксе.
\begin{thm}[фон-Неймана о минимаксе]
Пусть $f(x,y)$, $x\in \Omega_1, y\in \Omega_2$. $f(x,y)$ выпукла по $x$, вогнута по $y$. $\Omega_1, \Omega_2$ выпуклые и хотя бы одно компактно, $f$ непрерывна. Тогда 
$$ \min\limits_{x \in \Omega_1} \max\limits_{y \in \Omega_2} f(x,y) = \max\limits_{y \in \Omega_2} \min\limits_{x \in \Omega_1} f(x,y)$$
\end{thm}

Далее перепишем задачу на поиск минимального собственного числа информационной матрицы плана:
 $$\lambda_{\min}(M) = \min \limits_{||p||=1} \Tr{p}Mp$$
  Пусть $p_i$ — ортонормированный базис. Тогда 
$$p \sum \sqrt{\alpha_i} p_i$$
$$ \sum \alpha_i = 1$$
Отсюда 
\begin{equation}
\begin{split}
&\min \limits_{||p||=1} \Tr{p}Mp = \sum \limits_{i=1}^m \alpha_i\Tr{p_i}Mp_i \\
&\trace (M \sum \alpha_i p_i \Tr{p_i}) = \trace{MA}
\end{split}
\end{equation}
где $A = \sum \limits_{i=1}^m \alpha_i p_i\Tr{p_i}. \trace{A} = 1$. Таким образом в старых обозначениях:
$$\lambda_{\min}(M) = \min \limits_{A \in \mathbb{A}} \trace{MA}$$

Теперь объединим предыдущее разложение и теорему об минимаксе:
$$\lambda_{\min}M(\xi) = \min_{||p||=1}\Tr{p}M(\xi)p = \min_{A \in \mathbb{A}} \trace{AM}$$
Нам интересна $E$-оптимальность и поэтому промаксимизируем по всем планам. Пусть $\mathbb{M} = \{ M(\xi) \}$ — множество информационных матриц планов (оно выпуклое и компактное). Таким образом\footnote{Мы воспользовались тем, что $\trace{AB}$ является линейной выпокло-вогнутой фнукцией. Кроме того при переходе от
$\max\trace{A\sum f(x_i)\Tr{f(x_i)}w_i}$ использовалось то, что максимум выпуклой сумма положительных слагаемых не превосходит максимального из них}:
\begin{equation}
\begin{split}
&\max\limits_{\xi}\min\limits_{A}\trace{AM} = \max{M \in \mathbb{M}}\min\limits_{A \in \mathbb{A}} = \\
&\min\limits_{A \in \mathbb{A}}\max\limits_M\trace{AM} = \min\limits_{A}\max\limits_{\xi}\trace{A\sum f(x_i)\Tr{f(x_i)}w_i} = \min\limits_{A}\max\limits_{x}\Tr{f(x)}Af(x)
\end{split}
\end{equation}

Выкладка про то, что из минимакса следует эквивалетность:
$$ \min\limits_{A} \max\limits_{X} \leq \Tr{f}A^{*} f \leq \lambda_{\min}M(\xi) \leq \max\limits_{\xi}\lambda_{\min}M(\xi)$$
Последнее и первое по теореме о минимаксе равны.

$$ \eta(x, \theta) = \alpha_0 + \sum\limits_{i=1}^k \alpha_i \sin(x) + \beta_i \cos(x) + \varepsilon$$
Параметры входят линейно и $f(x) = \Tr{(1, \sin x, \cos x, …, \sin kx, \cos kx)}$, где $x \in [0, 2\pi]$.  

\begin{thm}
$E$-оптимальным планом для тригонометрической модели является 
$$ \xi^{*} = \left\{ \frac{2\pi(i-1)}{n}, i = 1…n\right\}$$
c весами $\frac{1}{n}$, где $n \geq 2k + 1$.
Матрица $M(\xi) = \diag(1, \frac{1}{2}, …, \frac{1}{2})$
\end{thm}
\begin{proof}
Рассмотрим $A^{*}=\diag(0,\frac{1}{2k}, …, \frac{1}{2k}$
$$\Tr{f}A^*f = \sum\limits_{j=1}^k \left(\frac{sin^2 jx + cos^2jx}{2k}\right)= k \frac{1}{2k} = \frac{1}{2}$$
Следовательно $\max\limits_x \Tr{f(x)}A^{*}f(x) = \frac{1}{2}$. Отсюда по теорем эквивалентности $M(\xi) = \diag(1, \frac{1}{2}, …, \frac{1}{2})$ является матрицей оптимального плана. 

Оптимальный план будет в точках $\{ \frac{2\pi(i-1)}{n}\}$. {\color{blue} как это выводится не ясно, но то, что результат будет тем, что надо кажется достаточно известный факт.}. 
{\color{blue} Как, кажется, можно доказать: берем какую-то из сумм вида $\sum w_k \sin\frac{2\pi(i-1)s}{n} \cos\frac{2\pi(j-1)s}{n}$. Если все веса одинаковы, то после какой-то перегруппировки тут будет сумма $a$ и $-a$. Для диагонали, наоборот, будет сумма $\sin^2(x) + \cos^2(x)$ которые будут давать 1. Но это надо аккуратно проверять.} 
\end{proof}

\begin{note}
$E$-оптимальный план не обязательно единственный. 
\end{note}

\subsection{Теорема о структуре матрицы из условия эквивалентности.}
\begin{thm}
В условия теоремы эквивалентности:
$$ A^{*} = \sum \limits_{i=1}^s \alpha_i p_i \Tr{p_i}$$
где $p_1, …, p_s$ —  ортонормированный базис, $\alpha_i >0$, $\sum\alpha_i = 1$. 
$s$ равно кратности минимального собственного числа.
\end{thm}
\begin{proof}
$A^*$ — неотрицательно-определенная матрица. Следовательно существует ортонормированный базис из собственных векторов. Выберем его так, что первые $s$ векторов соответствуют минимальному собственному числу $M$. Тогда для любого $p$:
\begin{equation}
\begin{split}
&p = \sum \alpha_i p_i \\
&A^{*} = \sum \alpha_i p_i \Tr{p_i}  = \sum\limits_{i=1}^{s} \alpha_i p_i \Tr{p_i} + \sum\limits_{i=s+1}^m \alpha_i p_i \Tr{p_i}\\
&\max\limits_x\Tr{f(x)} A^{*} f(x) = \max\limits_{x}\trace{\sum\limits_i  \alpha_i \Tr{p_i} f(x)\Tr{f(x)} p_i} \\
& = \max\limits_{\xi}\trace{A M(\xi)} = \max \limits_{\xi}\trace{\sum \alpha_i \Tr{p_i}Mp_i} \\
\trace{\sum \alpha_i \Tr{p_i}Mp_i} = \trace{\sum \alpha_i \lambda_i}
\end{split}
\end{equation}
Смотрим на последние два равенство. В последнем мы воспользовались тем, что $p_i$ собственные вектора $M$. Далее т.к. $a_i$ задают выпуклую комбинацию, то минимум достигается, если не нулевые $\alpha_i$  будут только среди тех коэффициентов, которые стоят передем минимальным собственным числом, что нам и требовалось.
\end{proof}

\begin{note}
Рассмотрим частный случай $s=1$. Тогда $A = p \Tr{p}$. 
$$\max\limits_{x} f(x) p \Tr{p} f(x) \leq \lambda_{\min}$$
Что тоже самое, что 
$$\max\limits_{x} (\Tr{p}f(x))^2 \leq \lambda_{\min}$$
\end{note}

\subsection{Теорема о числе опорных точек в E-оптимальных планах для полиномиальных моделей.}
Рассмотрим полиномиальную модели:
\begin{equation}
\eta(x,\theta)  = \Tr{\theta}f(x), x \in [a,b]
\end{equation}
где
$$\theta = \Tr{(\theta_0, …, \theta_{m-1})}  \in \mathbb{\Theta}$$
$$f(x) = \Tr{(1, x, …, x^{m-1})}$$

\begin{thm}
Пусть $m > 2$. Тогда существует единственный $E$-оптимальный план с $m$-опорными точками, две из которых совпадают с граничными точками.
\end{thm}
\begin{proof}
Пусть $\xi^* = \begin{pmatrix} x_1 & … & x_n \\ w_1 & … & w_n \end{pmatrix}$ — оптимальный план. Тогда по теореме эквивалентности $\exists A^{*}$ такое, что $=$
\begin{equation}
\Tr{f(x)}A^{*}f(x) \leq \lambda_{\min} \\
\Tr{f(x_i)}A^{*}f(x_i) \leq \lambda_{\min}
\end{equation}

\begin{equation}
\begin{split}
g(x) = \Tr{f(x)}A^{*} f(x) - \lambda_{\min} \\
g(x) \leq 0
\end{split}
\end{equation}

$g(x)$ является полиномом степени $2n-2$. При этом у него имеется по крайне мере $n$ корней на $[a,b]$. За счет того, что полином не может стать больше, внутренние корни должны иметь кратность $2$. Далее на бесконечностях полином стремится к $+\infty$ или $g(x) =0$. Пусть на бесконечности полином стремится к $+\infty$. Тогда у $g(x)$
будет по крайне мере  $2(n-2)+2$ нуля в случае, когда будет по точке на краях. В противном случае степень полинома будет больше и такая ситуация не возможна.  
Носители всех планов совпадают, т.к. мы можем считать, что $A^{*}$ фиксированная,  а план $\xi^{*}$ варьировать.  В итоге для всех возможных $\xi^{*}$ будем получать, что полином $g(x)$ один и тот же. Следовательно носителя плана совпадает с его корнями. Отсюда получаем, что существование нескольких решений возможно только в случае, когда $g(x) = const$\footnote{Существует всего один ненулевой полином степени $p$ с $p$ нулями.}.
Пусть $g(x) = const$. Тогда $\Tr{f}A^{*}f = \sum b_i x^i$,  где все $b_i$, кроме $b_0$ равны нулю. Таким образом, получаем, что\footnote{квадратичная формула обнуляется на всех векторах, кроме первого орта. Отсюда следует, что все элементы матрицы, кроме (1,1) должны быть нулем. Это не очень сложное упражнение по линейной алгебре, которое скорее всего неоднократно доказывалось.} 
$$ A^{*} = \begin{pmatrix} 1 & 0 & … & 0 \\ 0 & 0 & … & 0 \\ … & … & … & … \end{pmatrix}$$
$$A^{*} = e_1 \Tr{e_1}$$
Следовательно $Me_1 = \lambda e_1$ (пользуемся теоремой о структуре матрицы из условия эквивалентности). При этом матрицу $M$ мы можем вычислить:
$$ M = \begin{pmatrix} 1 & \sum w_ix_i & \sum w_ix_i^2 & … \\ \sum w_i x_i & … & … & …\\  … & … & … & … \end{pmatrix} $$

Ситуация, когда $Me_1 = \lambda e_1$ возможна только для $m=1,2$. Таким образом, для $m > 2$ решение единственно. 
\end{proof}


  \begin{note}
  Стоит отметить, что в данной теореме мы не доказали то, что план будет единственный. Мы доказали, что опорные точки плана являются единственными. 
 \end{note}

\subsection{Теорема о E-оптимальных планах для линейной модели на произвольном отрезке}
Рассмотрим линейнуюю модель\footnote{Является исключением из предыдущего пункта $m=2$} $\eta(x, \theta) = \theta_0 + \theta_1 x$, $x \in r_1,r_2]$. 
\begin{thm}
\begin{enumerate}
\item Если $r_1r_2 \geq -1$, то $\xi = \begin{pmatrix} r_1 & r_2 \\ w_1 & w_2\end{pmatrix}$, где 
$$w_1 = \frac{2+r_1^2 + r_1r_2}{4+(r_1+r_2)^2}$$
$$ w_2 = \frac{2+r_1^2 + r_1r_2}{4+(r_1+r_2)^2}$$ 
\item Если $r_1r_2 < -1$
$$\xi_{a,b} = \begin{pmatrix} a & b \\ \frac{b}{b-a} & \frac{-a}{b-a} \end{pmatrix}$$
где $r_1 \leq a < 0$, $0 < b \leq r_2$, $|ab|> 1$
\end{enumerate}
\end{thm}
\begin{proof}
$$ M = \begin{pmatrix} 1 & w_0 x_0 + w_1 x_1 \\ w_0 x_0 + w_1 x_1 & w_0 x_0^2 + w_1 x_1^2 \end{pmatrix}$$
$$ a \frac{b}{b-a} + b \frac{-a}{b-a} = 0$$
$$ a^2 \frac{b}{b-a} + b^2 \frac{-a}{b-a} = -ab$$
Следовательно: 
$$M(\xi_{a,b}) = \begin{pmatrix} 1 & 0 \\ & 0 & -ab \end{pmatrix}$$

Собственные числа матрицы
$$\begin{pmatrix} 1 & c \\ c & d \end{pmatrix}$$ 
равны
$$\lambda_1 = 1/2 (-\sqrt(4 c^2+d^2-2 d+1)+d+1)$$
$$ \lambda_2 = 1/2 (\sqrt(4 c^2+d^2-2 d+1)+d+1)$$
Совпадают они только в случае, когда $c=0$ и $d=1$. 
Если $d$ фиксировано, то максимальное $\lambda_{\min}(M)$ будет достигаться при $c = 0$. При $c=0$ минимальным собственным числом будет 1, если $d>1$. Если $d< 1$, то этим собственным числом будет $d$. Мы хотим максимизировать минимальное собственное число, а значит нам нравится вариант $d\geq 1$. При сформулированных в теореме условиях это будет выполнено. Отсюда получаем, что для $r_1r_2 < -1$ у нас есть много оптимальных планов.

Теперь рассмотрим случай $r_1r_2 \geq -1$. Предположим, что минимальное собственное число информационной матрицы оптимального плана имеет единичную кратность. Тогда
$$A^{*} = q\Tr{q}$$
где  $q$ собственных вектор $M$, $||q|| = 1$.
Матрица $M(\xi) = f(r_1)\Tr{f(r_1)}w + f(r_2)\Tr{f(r_2)}(1-w)$ 

Будем искать $q$ в виде $\Tr{(1, q_1)}$. Тогда, если $r_1$ и $r_2$ точки плана, то
$$ 1 + q_1r_1 = ±\sqrt{\lambda_{\min}}$$
$$ 1 + q_1r_2 = ±\sqrt{\lambda_{\min}}$$
Из уравнения видим, что в уравнения не могут давать один знак, поэтому сложим их и получим:
$$2 + q_1(r_1+r_2)=0$$
$$q_1 = \frac{-2}{r_1+r_2} = -\frac{1}{\mu}$$
Проверим, что такой $q$ является собственным вектором:
\begin{equation}
\label{eigeq}
\left(f(r_1)\Tr{f(r_1)}w + f(r_2)\Tr{f(r_2)}(1-w)\right)q = \lambda q
\end{equation}
Пусть 
$$ 1 + r_1 q_1 = -h$$
$$ 1 + r_2 q_1 = h = 1 - \frac{r_2}{\mu}$$ 
Тогда \eqref{eigeq} можно записать в виде
$$ f(r_1) h w + f(r_w)(-h)(1-w) = \lambda \begin{pmatrix} 1 \\ -\frac{1}{\mu} \end{pmatrix}$$
Подставляем и получаем\footnote{Мы можем считать, что $w$ у нас есть в условии доказываемой теоремы, но он также получается и просто из системы. После того, как $w$ найден, поиск $\lambda$ сводится к подстановке и алгебраическим манипуляциям}
$$ \lambda = (2w - 1)h = \frac{r^2}{r^2 + \mu ^2}$$ 
где $r = \frac{r_2 - r_1}{2}, \mu = \frac{r_2+r_1}{2}$

Можно проверить\footnote{вычислив собственные числа матрицы}, что найденное $\lambda$ будет с.ч. кратности 1, что дает нам оптимальность (по теореме эквивалентности).
 \end{proof}

 \subsection{Теорема о E-оптимальных планах для квадратичной модели на симметричном отрезке}
 Рассмотрим $\eta(x,\theta) = \theta_0 + \theta_1 x + \theta_2 x^2 + \varepsilon, x \in [-r, r]$. 
 \begin{thm}
 Для квадратичной регрессии на симметричном промежутке существует единственный $E$-оптимальный план
 $$\xi^{*}  = \begin{pmatrix} -r & 0 & r \\ w & 1 -2w & w\end{pmatrix}$$
 где при $r\leq \sqrt{2}$
 $$w = \frac{1}{1+r^4}, \lambda^{*} = \frac{r^4}{4+r^4}$$
 при $r\geq\sqrt{2}$
 $$ w = \frac{r^2 - 1}{2r^4}, \lambda^{*} = \frac{r^2 - 1}{r^2}$$
 \end{thm}
 \begin{proof}
Из теоремы о числе точек получаем, что у нас 2 точки на концах. Из симметрии и единственности получаем, что третья точка должна быть нулем:
Пусть $\xi$ — план с точкой $x \neq 0$. Тогда $\tilde{x}$ план с $x\rightarrow -x$. Рассмотрим $\xi^{*} = \frac{\xi+\tilde{xi}}{2}$. 
$$\lambda_{\min}M(\frac{\xi+\tilde{\xi}}{2}) < \frac{\lambda_{\min}M(\xi) + \lambda_{\min} M(\tilde{\xi})}{2}$$
$$ \begin{pmatrix} m_1 & c \\ c & m_2 \end{pmatrix} + \begin{pmatrix} m_1 & -c \\ -c & m_2 \end{pmatrix}$$
И для суммы в последней строчки мы в прошлой теореме уже выясняли, что максимум когда матрица диагональная. 
Следовательно план симметричный\footnote{В том числе веса на концах совпадают.}. Получаем, что матрица $M(\xi)$ имеет вид:
$$M(\xi) = \begin{pmatrix} 1 & 0 & 2r^2w \\ 0 & 2r^2 w & 0 \\ 2r^2w & 0 & 2wr^4 \end{pmatrix}$$
$$\det (M - \lambda I) = 0$$
$$\det(M-\lambda I) = \left(2r^2 w - \lambda\right)\left(\lambda^2 - \lambda(1 + 2r^4 w)+2r^4w - 4r^4w^2\right)$$
$$\lambda = 2r^2 w$$
$$\lambda(w) = \frac{1 + 2r^4w ± \sqrt{(1+2r^2w)^2 - 8r^4w + 16r^rw^2}}{2}$$
Нам интересно минимальное собственное число и считаем, что в последнем слагаемом минус. Тогда минимум получившегося выражения можно найти с помощью дифференцирования и приравнивания производной к нулю. После этого будет найдено оптимальное $w$\footnote{Надо помнить о том, что $w$ является коэффициентом из выпуклой комбинации и лежит в $[0,1]$. Тем не менее, точки локальных минимумов/максимумов данной функции оказываются хорошими}
Найдя экстремумы относительно $w$ получим\footnote{В репозитории лежит файл для Mathematica, в котором все это считается. На бумажке это считать как-то сложновато}:
$$ w((r^4+4)w - 1) = 0$$
$$w = \frac{1}{r^4 + 4}$$
Решение $w=0$ дает собственное число $\frac{1}{2}$, которое больше, чем $2r^2w$.  
В итоге 
$\lambda = \frac{r^4}{4+r^4}$
Нас интересует минимальное собственное число. Первое собственное число — линейная по $w$ функция. Вторая, как мы видим, сначала возврастает по $w$, а затем убывает. В нуле график прямой лежит ниже, чем график второго собственного числа. Таким образом, если точка $w=\frac{1}{r^4 + 4}$ лежит после пересечения графика $2r^2w$ и 
$\frac{1}{2} \left(2 r^4 w-\sqrt{4 r^8 w^2+16 r^4 w^2-4 r^4 w+1}+1\right)$, то именно она будет точкой плана, в противном случае точкой плана будет пересечение этих графиков.
Точкой пересечения графиков является
$$w=\frac{r^2-1}{2 r^4}, r > 1$$
Откуда можно получить, что для $0 \leq r \leq \sqrt{2}$ решением будет 
$$w=\frac{1}{r^4 + 4}$$
 \end{proof}



 \subsection{Теорема о кратности собственных чисел информационных матриц для полиномиальных моделей}
 \begin{thm}
Пусть $\xi$ — любой невырожденный план. $\lambda_{\min}(M) > 0$. Тогда кратность любого собственного числа информационной матрицы не превосходит двух.
 \end{thm}
 \begin{proof}
Пусть у матрицы $M(\xi)$ есть некоторое собственное число кратности $s > 2$. Выберем $p_1,…p_s$ и $p_{s+1}…p_m$ — ортонормированный базис из собственных векторов $M(\xi)$, где первые $s$ векторов соотвествуют собственному числу, кратность которого больше двух. Первые $s$ собственных векторов можно выбрать таким образом, что
$\Inner{p_i}{f(x)}$ будет многочленом степени не более, чем $m-i$. Введем функции $\psi_1$ и $\psi_2$:
$$ \psi_1(x) = \Tr{p_2}f(x)$$
$$ \psi_2(x) = \Tr{p_3}f(x)$$
Теперь рассмотрим функции
$$\Tr{p_i}f(x) \psi_1(x), i = 1, 4,5,…m$$
$$\Tr{p_i}f(x) \psi_2(x), i = 1, 4,5,…m$$
$p_i$ являются собственными векторами информационной матрицы, поэтому
$$\Tr{p_i} \sum w_k f(x_k)\Tr{f(x_k)}p_j = 0$$
если $i \neq j$. Отсюда имеем
$$ \sum (p_i f(x_k)) \psi_j(x_k) w_k = 0$$
А также 
$$\sum \psi_1(x_k)\psi_2(x_k) w_k = 0$$
$$\sum (\psi_1(x_k)^2 - \psi_2(x_k)^2) w_k = 0$$
Утверждается, что эти $\Tr{p_i}f(x)\psi_j(x)$, $\psi_1(x)\psi_2(x)$ и $\psi_1^2(x) - \psi_2^2(x)$ являются линейно-независимыми многочленами\footnote{Понять, почему это правда и дописать}. Кроме того, степень этих многочленнов не превосходит $2m-3$\footnote{проверяется вычислением максимальной степени у этих полиномов}. При этом многочленов выписано $2m-2$. Значит они образуют базис и могут выразить любой многочлен степени, не превосходящей $2m-3$. В частности, $\psi(x) = const = 1.$. Обозначим получившиеся базисные функции за $\phi_i(x)$. Тогда 
$$\psi(x) = \sum \alpha_i \phi_i(x) = const$$
$$const = \sum w_k \psi(x_k) = \sum \sum \alpha_i w_k \phi_i(x_k) = 0$$
Полученное противоречие доказывает теорему.
 \end{proof}


 \subsection{Теорема о простоте минимального собственного числа для полиномиальных моделей}
 \begin{thm}
Рассмотрим полиномиальную модель на  $[-r,r]$, $m > 2$, $r \leq 1$. Пусть $\xi$ некоторый план. Тогда кратность $\lambda_{\min}$ равна 1.
 \end{thm}
\begin{proof}
Допустим это не так. Рассмотрим $\xi$ с числом опорных точек $n \geq m$ и кратностью $\lambda_{\min} = 2$. Тогда 
существует собственный вектор $p$, соответсвующий $\lambda_{\min}(M(\xi))$. ИЗ предыдущей теоремы мы знаем, что его можно взять вида $p = \Tr(p_0, …, p_{m-2}, 0)$. Рассмотрим $\tilde{p} = \Tr{(0, p_0,…p_{m-2})}$. Далее
\begin{equation}
\begin{split}
&\tilde{p}M(\xi)\tilde{p} = \int\limits_{-r}^r \Tr{\tilde{p}} f(x) \Tr{f(x)} \tilde{p}d\xi(x) = \\
&\int\limits_{-r}^r x^2 \Tr{p} f(x)\Tr{f(x)} p d\xi(x) < \int  \limits_{-r}^r  \Tr{p} f(x)\Tr{f(x)} p d\xi(x)= \\
&\Tr{p}M(\xi)p = \lambda_{\min}(M(\xi))
 \end{split}
 \end{equation}
 Но по определению/свойству минимального собственного числа для любого вектора единичной длинны $q$
 $\Tr{q} M(\xi)q \geq \lambda_{\min}M(\xi)$. Противоречие.
\end{proof}


\subsection{Локально E-оптимальные планы для модели Михаэлиса-Ментен}
{\color{blue} Простейшая дробно-линейная модель переместилась после этого билета. В ней рассуждения проводятся в том же духе, но проще. }
Рассмотрим локально $E$-оптимальные планы для модели Михаэлиса-Ментена. Пусть 
$\eta(x, \theta) = \frac{ax}{x+b}$, $x\in[0,d]$, $b > 0$. Как мы уже выясняли, после линеаризации получаем
$$f_1(x) = \frac{x}{x+b}$$
$$f_2(x) = \frac{-ax}{(x+b)^2}$$
Искать оптимальный план будем, как обычно, с помощью теоремы эквивалентности. Пусть $\lambda = \lambda_{\min}(M(\xi^{*}))$, где $\xi^{*}$ некоторый оптимальный план. Тогда существует $A^{*}$ такая, что
\begin{equation}
\begin{split}
&(\frac{x}{x+b}, \frac{-ax}{(x+b)^2}) A^{*}\begin{pmatrix} \frac{x}{x+b}\\ \frac{-ax}{(x+b)^2}\end{pmatrix} = \\
& \alpha_1 \frac{x^2}{(x+b)^2} - \alpha_2 \frac{ax^2}{(x+b)^3} + \alpha_3\frac{a^2 x^4}{(x+b)^4} \leq \lambda \\
\alpha_1 + \alpha_3 = 1\\
\alpha_1 \alpha_3 - (\frac{\alpha_2}{2})^2 > 0, \alpha_1 > 0
\end{split}
\end{equation}

Условия на $\alpha$ берутся из положительной определенности и $\trace{A^*}=1$. Далее обозначим
$$g(x) = \alpha_1x^2(x+b)^2  - \alpha_2 x^2 (x+b) + \alpha_3 a^2 x^2 - \lambda (x+b)^4 = 0$$
Как обычно, берем производные и получаем, что у $g(x)$ будет не более четырех нулей с учетом кратности. По теореме эквивалентности в точках плана $g(x_i) = 0$. При этом $g(x) \leq 0$ для всех $x \in [0,d]$, а значит 
внутренние нули — кратности два. Потому максимальное количество точек плана будет равно 3 и план состоит либо из трех, либо из двух точек. Можно показать (проверяется вычислением), что коэффициент при $x^4$ больше либо равен нуля. Следовательно $g(x)$ стремится к бесконечности. Рисуем картинку и получаем, что в таком случае возможна лишь одна ситуация — одна опорная точка является внутренней, а вторая совпадает с $d$. 

Таким образом, оптимальный план имеет вид
$$\xi = \begin{pmatrix} x & d \\ w & 1 -w \end{pmatrix}$$
Требуется найти $x$ и $w$.

Исследуем теперь кратность $\lambda_{\min}$. Если она равна $1$, то $A^{*}= \Tr{p}p$ из теоремы о структуре матрицы. 
\begin{equation}
M(\xi) = \begin{pmatrix}
wf_1^2(x_1) + (1-w)f_1^2(x_2) & wf_x(x_1)f_2(x_1) + (1-w)f_1(x_2)f_2(x_2) \\
… & wf_2^2(x_1) + (1-w) f_2^2(x_2) \\ 
\end{pmatrix} = \begin{pmatrix} u & v \\ v & w \end{pmatrix}
\end{equation}

Далее 
\begin{equation}
\begin{split}
\det\left(M(\xi) - \lambda I\right) = 0 \Rightarrow \\
\det \begin{pmatrix} u - \lambda  & v \\ v & w - \lambda\end{pmatrix} = (u - \lambda)(w-\lambda) - v^2  = \\
\lambda^2 - \lambda(u + w) + wu - v^2 
\end{split}
\end{equation}
Решив, получим, что собственные числа равны
\begin{equation}
\begin{split}
&\frac{1}{2} \left(-\sqrt{u^2-2 u w+4 v^2+w^2}+u+w\right)\\
&\frac{1}{2} \left(\sqrt{u^2-2 u w+4 v^2+w^2}+u+w\right)
\end{split}
\end{equation}

Отсюда кратность $\lambda_{\min}= 2$, если корень равен нулю. 
Выражение под корнем  можно записать, как 
$$(u- w)^2 - 4v^2  = 0$$ 
Следовательно $u=w$ и $v=0$ является условием, когда у матрицы 2 на 2 будет собственное число кратности 2.
Прямым вычислением проверяется, что в нашей ситуации это не так (для любого плана из двух точек)\footnote{Если посмотреть на $f_1$ и $f_2$ видно, что $v$ будет выпуклой комбинацией отрицательных слагаемых.}
Таким образом, получаем, что
$$A^{*} = p\Tr{p}$$
где $p$ соответсвует $\lambda_{\min}$ оптимального плана.

$\Tr{f}A^{*} f = (\Tr{p}f)^2$
\begin{equation}
\begin{split}
\Tr{p}f(d) = \lambda \\
\Tr{p}f(x)  = \lambda\\
\frac{\partial(\Tr{p}f(x))}{\partial x} = 0
\end{split}
\end{equation}
Нам конкретное $p$ не интересно и поделив его на $\lambda$ получаем систему:
\begin{equation}
\begin{split}
\Tr{p}f(d) = -1\\
\Tr{p}f(x)  = 1\\
\frac{\partial(\Tr{p}f(x))}{\partial x} = 0
\end{split}
\end{equation}
Получили систему из трех уравнений с тремя неизвестными ($p_2 \rightarrow -ap_2$):
\begin{equation}
\begin{cases}
p_1\frac{d}{d+b} + p_2 \frac{d}{(d+b)^2} = -1 \\
p_1 \frac{x}{x+b} + p_2 \frac{x}{(x+b)^2} = 1 \\ 
P_1 \frac{b}{(x+b)^2} + p_2 \frac{b-x}{(x+b)^2} = 0
\end{cases}
\end{equation}
Решение этой системы:
$$x = \frac{db}{\sqrt{2}(d+b) + b} $$
Далее остается найти $w$
Его можно найти из  
$$Mp = \lambda p $$
Решением будет 
$$w = \frac{\frac{\sqrt{2}a^2}{b (\sqrt{2}b - (4-3\sqrt{2})d)}}{2(d+b)^2 + \frac{a^2}{b^2(b\sqrt{2}-(4-3\sqrt{2})d)}}$$

\subsection{Локально E-оптимальные планы для простейшей дробно-рациональной модели}
Найдем оптимальные планы для простейшей дробно-рациональной модели:
$$\eta(x,\theta) = \frac{a}{x+b}$$
$$f_1(x) = \frac{1}{x+b}$$
$$f_2(x) = \frac{-a}{(x+b)^2}$$
где $x \in [0,d]$.

Для данной модели опорные точки будут не зависеть от параметров плана, если для оптимального плана кратность собственного числа равна 1. В предыдущем разделе показано, что кратность собственного числа в модели Михаэлиса-Ментена равна единице, аналогичные рассуждения показывают, что и для дробно-рациональной модели $s=1$. Кроме того, план надо искать для двух точек.
Повторив шаги из предыдущего билета получим систему\footnote{Я пока не проверял, что действительно такая. Но это всего-лишь вычисления, идейных сложностей нету.}
\begin{equation}
\begin{cases}
\frac{p_1}{x_1 + b} + \frac{p_2}{(x_1+b)^2} = -1\\
\frac{p_1}{x_1 + b} + \frac{p_2}{(x_2+b)^2} = 1\\
\frac{-p_1}{(x_1+b)^2} + \frac{2ap_2}{((x_2+b)^3} = 0
\end{cases}
\end{equation}
Предлагается искать план с точкой $x_1= 0$, $x_2 = x$\footnote{Не понятно, действительно ли план такой или мы просто «угадываем» одну из точек}. 
Получим систем, из которой $x = \sqrt{2}b$.
Далее непосредственными вычислениями находим $w$. 

\subsection{Многочлен наилучшего приближения. Теорема Чебышева}

Пусть $\phi(x), f_1(x), …, f_n(x)$ — непрерывные на $[a,b]$ функции c $l_{\infty}$-нормой
$$||\phi|| = \max\limits{a \leq x \leq b}|\phi(x)|$$ и $f_1, …, f_n$ линейно-независимы.
Требуется найти наилучшее приближение функции $\phi(x)$ с помощью линейной комбинации $f_1(x), …, f_n(x)$
$$ \alpha = \argmin\limits_{\alpha \in \R^n} ||\phi(x) - \Inner{\alpha}{f}||_{\infty}$$
$\sum a_i f_i(x)$ будем называть многочленом наилучшего приближения. Он всегда существует:
\begin{proof}
Функция $h_\phi(\alpha) = ||\phi  - \Inner{\alpha}{f}||$ непрерывна. Следовательно $h_{0}(\alpha)$ на единичной сфере достигает минимума $\varepsilon > 0$ (если минимум равен нулю, то $f_1,…f_n$ линейно-зависимы, что противоречит предположению). Следовательно
$$||\alpha_1 f_1 + … + \alpha_nf_n||= h_0(a) \geq ||a|| \varepsilon$$
Рассмотрим шар радиуса $R = \frac{2||\phi||}{\varepsilon}$. $h_{\phi}(\alpha)$ достигает минимума на этом шаре. Вне этого шара будет выполнено:
$$ h_\phi(\alpha_1, …, \alpha_n) \geq ||\Inner{\alpha, f}|| - ||\phi|| > R \varepsilon - ||\phi|| = ||\phi||$$
Следовательно вне этого шара значения целевой функции будут не меньше, чем внутри. Следовательно найденный минимум и есть то, что нам надо — многочлен наилучшего приближения. 
\end{proof}

Рассмотрим теперь частный случай $f_1, …, f_n$ — система Чебышева. Далее будем обозначать многочлен $\Inner{\alpha}{f}$ за $g_{\alpha}(x)$. 
\begin{thm}
Для произвольной непрерывной $\phi(x)$ на $[a,b]$ и системы Чебышева $f_1, …, f_n$ многочлен наилучшего приближения является единственным и характеризуется тем, что существует $n+1$ точки, такие что
$$\phi(x_j) - \sum \alpha_i f_i(x_j) = (-1)^{j}L$$
где $L = \argmin \limits_{\alpha}||\phi - \Inner{\alpha}{f}||$
\end{thm}  
\begin{proof}
Начнем с доказательства того, что если такой многочлен существует, то он является многочленом наилучшего приближения. Пусть $g_{\alpha}(x)$ такой многочлен, а $x_1, …, x_{n+1}$ соответсвующие точки.  Разобъем $[a,b]$ на $n$ отрезков, на каждом из которых $g_{\alpha}$ изменяется от $f(x) + L$ до $f(x) - L$, где $L = ||\phi - g_{\alpha}||$.  Пусть $g_{\beta}$ такой многочлен, что $||\phi - g_\beta|| \leq L$. В таком случае многочлен
$g_{\alpha}- g_{\beta}$ будет иметь не менее $n$ нулей и по теореме о числе нулей\footnote{Было в первых вопросах} получим, что $g_{\alpha}-g_{\beta}$ является тривиальным многочленом.

Пусть теперь $g_\alpha$ — многочлен минимального приближения. Из непрерывности существует хотя бы одна точка, в которой достигается максимальное отклонение. Пусть $x_1, …, x_q$, $q < n$  такие точки, что $g_{\alpha}$ принимает в них поочередно значения $±L$. Следовательно существует $q-1$ точка $a < y_1 < … < y_{q-1} < b$, в которых
$\phi(y_i) - g_{\alpha}(y_i) = 0$. Далее будет выполнено
$$ L \geq \phi(x) - g_{\alpha}(x) \geq -L + \mu, x \in [a, y_1] \cup [y_2, y_3]…,$$
$$ L - \mu \geq \phi(x) - g_{\alpha} \geq -L, x \in [y_1, y_2] \cup [y_3, y_4]…$$
Далее по теореме из начала существует многочлен $\omega(x)$ такой, что все его нули на $(a,b)$ будут узловыми в точках $y_1,…,y_{q-1}$. Пусть $\delta = \frac{\mu}{2\max |\omega(x)|}$ и получим, что $|\phi(x) - g_{\alpha}(x) + \delta \omega(x)| < m$  для любой точки внутри $(a,b)$. На краях могут возникнуть проблемы, если $|\phi(a) - g_\alpha(a)|= L$ и $\omega(a) = 0$. Исправляем эту проблему с помощью $\omega_{1}$ такого, что $\omega_1(x)(f(x)) - g_\alpha(x)) > 0, x = a,b$. В итоге получаем многочлен, для которого 
$$ ||\phi - \tilde{g}|| < m$$
Получаем противоречие тому, что искомый многочлен был многочленом наилучшего приближения.
\end{proof}


