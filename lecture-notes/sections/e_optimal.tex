\section{E-оптимальные планы}
\subsection{Определение и статистический смысл}
Пусть $M(\xi)$ — информационная матрица плана. 
\begin{dfn}
Будем говорить, что план $\xi$ является $E$-оптимальным, если 
$$ \xi = \argmin\limits_{\lambda_{\min}(M(\xi))}, \text{ где}$$
$\lambda_{\min}(M(\xi))$ — минимальное собственное число $M(\xi)$. 
\end{dfn}
Статистический смысл этого критерия состоит в минимизации дисперсии следующего выражения:
$$D(\Inner{p}{\theta}), \text{ где } p \in \R^m, ||p||_2 = 1$$
$$D(\Inner{p}{\theta}) = \Tr{p}M^{-1}(\xi)p$$
Из последней формулы видно\footnote{вспоминаем линейную алгебру и то, что $M^{-1}$ является положительно-определенной матрицой}, что максимум этого выражения достигается на первом собственном векторе, а cам максимум равен первому собственному числу. Собственные числа $\frac{1}{\lambda_i}$ матрицы $M^{-1}$ совпадают с обратными к  $\lambda_i$ — собственным числам матрицы $M$. Из такого представления следует, что $E$-оптимальность означает минимизацию максимальной длины оси доверительного эллипсоида для МНК-оценки\footnote{Эта ось, как все помнят, совпадает с направлением первого собственного вектора матрицы $D(\theta)$.}.


{\color{blue} что сюда еще надо?}
\subsection{Теорема эквивалентности}
\begin{dfn}
Обозначим класс неотрицательно-определенных симметричных матриц с единичным следом за $\mathbb{A}$. 
$$\mathbb{A} = \{ A | A \text{ p.s.d.}, \trace{A} = 1 \}$$
\end{dfn}
\begin{thm}
Пусть $f(x)=\Tr{(f_1(x), …, f_n(x)}$, $x \in \mathbb{X}$ является непрерывной функцией. Тогда:
\begin{enumerate}
\item План $\xi^{*}$ является $E$-оптимальным тогда и только тогда, когда 
$$\exists A^{*} \in \mathbb{A} \max\limits_{x \in \mathbb{X}} \Tr{f(x)}Af(x) \leq \lambda_{\min}(M(\xi^*)$$
\item $$\Tr{f(x_i^*)}A^{*}f(x_i^*) = \lambda_{\min}(M(\xi^*)), \text{ где}$$
для $i=1…n$ $x_i^*$ являются опорными точками $E$-оптимального плана.
\item $$ \min\limits_{A}\max\limits_{x \in \mathbb{X}} \Tr{f(x)}Af(x) = \max\limits_{\xi}\lambda_{\min}M(\xi)$$
\end{enumerate}
\end{thm}


Эта теорема является следствием общей теоремы о минимаксе.
\begin{thm}[фон-Неймана о минимаксе]
Пусть $f(x,y)$, $x\in \Omega_1, y\in \Omega_2$. $f(x,y)$ выпукла по $x$, вогнута по $y$. $\Omega_1, \Omega_2$ выпуклые и хотя бы одно компактно, $f$ непрерывна. Тогда 
$$ \min\limits_{x \in \Omega_1} \max\limits_{y \in \Omega_2} f(x,y) = \max\limits_{y \in \Omega_2} \min\limits_{x \in \Omega_1} f(x,y)$$
\end{thm}

Далее перепишем задачу на поиск минимального собственного числа информационной матрицы плана:
 $$\lambda_{\min}(M) = \min \limits_{||p||=1} \Tr{p}Mp$$
  Пусть $p_i$ — ортонормированный базис. Тогда 
$$p \sum \sqrt{\alpha_i} p_i$$
$$ \sum \alpha_i = 1$$
Отсюда 
\begin{equation}
\begin{split}
&\min \limits_{||p||=1} \Tr{p}Mp = \sum \limits_{i=1}^m \alpha_i\Tr{p_i}Mp_i \\
&\trace (M \sum \alpha_i p_i \Tr{p_i}) = \trace{MA}
\end{split}
\end{equation}
где $A = \sum \limits_{i=1}^m \alpha_i p_i\Tr{p_i}. \trace{A} = 1$. Таким образом в старых обозначениях:
$$\lambda_{\min}(M) = \min \limits_{A \in \mathbb{A}} \trace{MA}$$

Теперь объединим предыдущее разложение и теорему об минимаксе:
$$\lambda_{\min}M(\xi) = \min_{||p||=1}\Tr{p}M(\xi)p = \min_{A \in \mathbb{A}} \trace{AM}$$
Нам интересна $E$-оптимальность и поэтому промаксимизируем по всем планам. Пусть $\mathbb{M} = \{ M(\xi) \}$ — множество информационных матриц планов (оно выпуклое и компактное). Таким образом\footnote{Мы воспользовались тем, что $\trace{AB}$ является линейной выпокло-вогнутой фнукцией. Кроме того при переходе от
$\max\trace{A\sum f(x_i)\Tr{f(x_i)}w_i}$ использовалось то, что максимум выпуклой сумма положительных слагаемых не превосходит максимального из них}:
\begin{equation}
\begin{split}
&\max\limits_{\xi}\min\limits_{A}\trace{AM} = \max{M \in \mathbb{M}}\min\limits_{A \in \mathbb{A}} = \\
&\min\limits_{A \in \mathbb{A}}\max\limits_M\trace{AM} = \min\limits_{A}\max\limits_{\xi}\trace{A\sum f(x_i)\Tr{f(x_i)}w_i} = \min\limits_{A}\max\limits_{x}\Tr{f(x)}Af(x)
\end{split}
\end{equation}

Выкладка про то, что из минимакса следует эквивалетность:
$$ \min\limits_{A} \max\limits_{X} \leq \Tr{f}A^{*} f \leq \lambda_{\min}M(\xi) \leq \max\limits_{\xi}\lambda_{\min}M(\xi)$$
Последнее и первое по теореме о минимаксе равны.

$$ \eta(x, \theta) = \alpha_0 + \sum\limits_{i=1}^k \alpha_i \sin(x) + \beta_i \cos(x) + \varepsilon$$
Параметры входят линейно и $f(x) = \Tr{(1, \sin x, \cos x, …, \sin kx, \cos kx)}$, где $x \in [0, 2\pi]$.  

\begin{thm}
$E$-оптимальным планом для тригонометрической модели является 
$$ \xi^{*} = \left\{ \frac{2\pi(i-1)}{n}, i = 1…n\right\}$$
c весами $\frac{1}{n}$, где $n \geq 2k + 1$.
Матрица $M(\xi) = \diag(1, \frac{1}{2}, …, \frac{1}{2})$
\end{thm}
\begin{proof}
Рассмотрим $A^{*}=\diag(0,\frac{1}{2k}, …, \frac{1}{2k}$
$$\Tr{f}A^*f = \sum\limits_{j=1}^k \left(\frac{sin^2 jx + cos^2jx}{2k}\right)= k \frac{1}{2k} = \frac{1}{2}$$
Следовательно $\max\limits_x \Tr{f(x)}A^{*}f(x) = \frac{1}{2}$. Отсюда по теорем эквивалентности $M(\xi) = \diag(1, \frac{1}{2}, …, \frac{1}{2})$ является матрицей оптимального плана. 

Оптимальный план будет в точках $\{ \frac{2\pi(i-1)}{n}\}$. {\color{blue} как это выводится не ясно, но то, что результат будет тем, что надо кажется достаточно известный факт.}. 
{\color{blue} Как, кажется, можно доказать: берем какую-то из сумм вида $\sum w_k \sin\frac{2\pi(i-1)s}{n} \cos\frac{2\pi(j-1)s}{n}$. Если все веса одинаковы, то после какой-то перегруппировки тут будет сумма $a$ и $-a$. Для диагонали, наоборот, будет сумма $\sin^2(x) + \cos^2(x)$ которые будут давать 1. Но это надо аккуратно проверять.} 
\end{proof}

\begin{note}
$E$-оптимальный план не обязательно единственный. 
\end{note}

\subsection{Теорема о структуре матрицы из условия эквивалентности.}
\begin{thm}
В условия теоремы эквивалентности:
$$ A^{*} = \sum \limits_{i=1}^s \alpha_i p_i \Tr{p_i}$$
где $p_1, …, p_s$ —  ортонормированный базис, $\alpha_i >0$, $\sum\alpha_i = 1$. 
$s$ равно кратности минимального собственного числа.
\end{thm}
\begin{proof}
$A^*$ — неотрицательно-определенная матрица. Следовательно существует ортонормированный базис из собственных векторов. Выберем его так, что первые $s$ векторов соответствуют минимальному собственному числу $M$. Тогда для любого $p$:
\begin{equation}
\begin{split}
&p = \sum \alpha_i p_i \\
&A^{*} = \sum \alpha_i p_i \Tr{p_i}  = \sum\limits_{i=1}^{s} \alpha_i p_i \Tr{p_i} + \sum\limits_{i=s+1}^m \alpha_i p_i \Tr{p_i}\\
&\max\limits_x\Tr{f(x)} A^{*} f(x) = \max\limits_{x}\trace{\sum\limits_i  \alpha_i \Tr{p_i} f(x)\Tr{f(x)} p_i} \\
& = \max\limits_{\xi}\trace{A M(\xi)} = \max \limits_{\xi}\trace{\sum \alpha_i \Tr{p_i}Mp_i} \\
\trace{\sum \alpha_i \Tr{p_i}Mp_i} = \trace{\sum \alpha_i \lambda_i}
\end{split}
\end{equation}
Смотрим на последние два равенство. В последнем мы воспользовались тем, что $p_i$ собственные вектора $M$. Далее т.к. $a_i$ задают выпуклую комбинацию, то минимум достигается, если не нулевые $\alpha_i$  будут только среди тех коэффициентов, которые стоят передем минимальным собственным числом, что нам и требовалось.
\end{proof}

\begin{note}
Рассмотрим частный случай $s=1$. Тогда $A = p \Tr{p}$. 
$$\max\limits_{x} f(x) p \Tr{p} f(x) \leq \lambda_{\min}$$
Что тоже самое, что 
$$\max\limits_{x} (\Tr{p}f(x))^2 \leq \lambda_{\min}$$
\end{note}