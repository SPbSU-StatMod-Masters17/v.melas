\section{Экспоненциальные модели с двумя параметрами. Построение локально-оптимальных планов}

{\color{blue} Кусок про экспоненциальные модели есть в сборнике (Пененко и т.д.), но несколько  с тем форматом, что был у нас на лекциях}

На протяжении нескольких следующих вопросов мы будем изучать экспоненциальную модель с двумя параметрами.
Она была введена в вопросе 4. Здесь кратко напомним ее, не уточняя деталей.
Пусть $y = \eta(x, \theta) + \varepsilon$, где 
$$\eta(x,\theta) = \sum\limits_{i=1}^{k} a_i e^{-\lambda_ix}, x \ge 0, k \in \mathbb{N}$$

$$M(\xi) = \sum \limits_{i=1}^{n} w_if(x_i)\Tr{f(x_i)}$$
$$\xi_{opt} = \argmax_{\xi} \det M(\xi)$$

Напомним, что локально-$D$-оптимальный план --- это такой, на котором достигается максимум определителя информационной матрицы, посчитанной в некотором приближении истинного решения.
Заметим, что в локально-оптимальном плане должно быть по крайней мере $2k$ точек (иначе ранг матрицы будет меньше $2k$, то есть количества параметров, и определитель будет нулем). 

Как мы покажем позже, функции $f$ образуют систему Чебышева, поэтому есть и верхняя граница на количество точек в оптимальном плане — $\frac{2k(2k+1)}{2}+1$ (Кажется, для систем Чебышева верхняя граница на самом деле $[\frac{2k(2k+1)/2 + 1}{2}]$, но мы это внятно не доказали. Мы получили теорему про $I(c)$, которая дает верхнюю границу для граничных точек (по модулю того, что у нас был конус, а нужно его сечение, но с этим можно бороться). Определитель матрицы будет гармонической функцией\footnote{там всякие попарные произведения, они после оператора лапласса умрут}, поэтому максимум у него на границе (вспоминаем матфизику), так что для $D$-оптимальных планов нам доказывать что-то про внутренность действительно не надо)


Для поиска локальных $D$-оптимальных планов мы будем пользоваться теоремой эквивалентности\footnote{Мы ее формулировали до этого, но пусть будет еще раз}:

\begin{thm}
Пусть $M$ — информационная матрица для параметра $\theta \in \Omega \subset \R^m$.
Пусть $d(x,\xi) = \Tr{f(x)}M^{-1}(\xi)f(x) = \mathbb D(\Tr{f(x)}\hat{\theta})$\footnote{
    Здесь мы пользуемся несмещенностью МНК-оценок и асимптотической нормальностью (таким образом, мы считаем выполнеными предположения вопроса 1, подробности там же). $\mathbb D(\Tr{f(x)}\hat{\theta}) = \mathbb E(\Tr{f}\hat{\theta} - \Tr{f}\theta)\Tr{(\Tr{f}\hat{\theta} - \Tr{f}\theta)} = \Tr{f}\mathbb E(\hat{\theta} - \theta)\Tr{(\hat{\theta} - \theta)}f = \Tr{f}\mathbb D_{\hat{\theta}}f \sim  \sigma^2\Tr{f}M^{-1}f / n$.}
%$\mathbb D_{\hat{\theta}} = \frac{\sigma^2}{n} M^{-1}$}


Эквивалентно:
\begin{itemize}
\item $\xi^{*}$ — $D$-оптимальный план (у нас локально);
\item $\xi^{*}$ — $G$\footnote{По всей видимости происходит от слова обобщенная (general).}-оптимальный план $\xi^{*} = \argmin_{\xi}\max_{x}d(x, \xi)$ (минимизирует максимальную дисперсию предсказаний);
\item $\max\limits_{x} d(x,\xi^{*}) = m$.
\end{itemize}
В опорных точках\footnote{То есть в точках, где дискретная мера отлична от 0.} $D$-оптимального плана  $d(x, \xi^{*})$ принимает свое максимальное значение.
\end{thm}

Эту теорему можно применять следующим простым способом: находим в каких точках $x$ функция $d$ достигает максимума, равного $m$ и получаем план, как дискретную меру, построенную по этим точкам.
В первом приближении именно так и будет происходить. Далее вопрос стоит в том, как нужно выбирать весовые коэффициенты для меры.
Для решения этой задачи нам потребуется следующее определение.

%Нам будут интересны специальные типы планов:
\begin{dfn}
План, число точек в котором совпадает с числом параметров, называется насыщенным
\end{dfn}

Для экспоненциальных моделей в большинстве случаев оптимальные планы являются насыщенными\footnote{\color{blue} Неясно.}.
Для дробно-рациональной модели, которую мы будем рассматривать в дальнейшем, все локально D-оптимальные планы будут насыщенными. Отметим важный факт о насыщенных планах\footnote{Именно этот факт оправдывает их использование}:
\begin{thm}
    Для насыщенных $D$-оптимальных планов все весовые коэффициенты одинаковы.
\end{thm}

\begin{proof}
$$ M(\xi) = \sum\limits_{i=1}^{m} w_i f(x_i)\Tr{f(x_i)} = FW\Tr{F}$$
$$ \det M(\xi) = \prod\limits_{i=1}^{m}w_i \det F\Tr{F} $$
Видно, что $w_i$ и $F$ можно максимизировать по отдельности.
Берем логарифм, вспоминаем правило множителей Лагранжа и получаем, что
$w_i = \frac{1}{m}$\footnote{можно и через неравенства между средним геометрическим и средним арифметическим доказать. ANDY update: второй способ, может быть, кому-то даже проще будет. Вспомните, когда в неравенстве между средним арифметическим
и средним геометрически достигается равенство. Это как раз тот случай.}.
\end{proof}
\begin{note}
    Утверждается, что такой план еще и единственный, но откуда это берется не ясно. {\color{blue} TODO}
\end{note}

Теперь отметим еще один полезный факт, связанный с экспоненциальной регрессией:
$$ \det(M(\xi, a, \lambda)) = C(a) \det\tilde{M(\xi, \lambda)}$$
Таким образом, оптимальный план не зависит от вектора $a$ и можно при поиске плана считать, что $a_i = 1$\footnote{Но нельзя считать, что у нас $k$ параметров, у нас их все равно $2k$, просто при максимизации мы можем считать $a_i=1$, т.к. они на выбор точек плана не влияют. 
    ANDY: Но не забывайте, что производные по $a_i$ в матрице нужно учитывать! Этот момент очень важен для дальнейшего понимания. Остановитесь на нем, пока не поймете.}


Для экспоненциальных систем производные будут образовывать систему Чебышева.
Этот факт мы доказывали в 4 вопросе.
Производные (с точностью до знака):
$$f_i(x) = e^{-\lambda_ix}, f_{2i} = x e^{-\lambda_i x}$$

Из них получаем множество функций $\{e^{-2\lambda_i x}, e^{-(\lambda_i + \lambda_j) x}, xe^{-(\lambda_i + \lambda_j) x}, x^2e^{-2\lambda_i x} \}$\footnote{Напомню, что именно попарные произведения образуют информационную матрицу.}

Перейдем к построению локально-оптимальных планов. Начнем с $k=1$. Тогда
$$\eta(x, \theta) = e^{-\lambda_1x}$$
$$f_1 = e^{-\lambda_1x}$$
$$f_2 = -xe^{-\lambda_1x}$$

\begin{thm}
При $k=1$ существует единственный $D$-оптимальный план
$$ \xi = \begin{pmatrix} 0 & \frac{1}{\lambda_1} \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$$
\end{thm}

\begin{proof}
По теореме эквивалентности $d(x, \xi) \leq 2 = m$.
Как всегда проводим линеаризацию в окрестности плана (см. вопросы 1-2, там это было).
Таким образом рассматриваем $\partial \eta(x, \theta) / \partial \theta_i$.
$$\Tr{F} = \begin{pmatrix} f_1(x_1) & f_1(x_2) \\ f_2(x_1) & f_2(x_2)\end{pmatrix}.$$

$$ W = \begin{pmatrix} \frac{1}{2} & 0 \\ 0 & \frac{1}{2} \end{pmatrix}.$$

Из этого простым перемножением матриц нетрудно получить, что
$$ M^{-1}(\xi) = (\Tr{F} W F) = \begin{pmatrix} 2 & -2\lambda_1 \\  -2\lambda_1 & 2 \lambda_1^2 (1 + e^2) \end{pmatrix}.$$
А из этого опять-таки перемножением матриц получается, что
$$ d(x, \xi) = \Tr{f(x)} M^{-1}(\xi) f(x) = e^{-2\lambda_1 x} (2 (1 - x \lambda_1) + 2(-x\lambda_1 + \lambda_1 x^2(1 +e^2)).$$
Ясно, что $0$ доставляет максимум этой функции. Далее можно непосредственно поставить $1 / \lambda_1$ и убедиться, что максимум достигается\footnote{\color{blue} Какая-то лажа где-то, пусть тот, кто хорошо считает, проверит.}
или же продифференцировать и посчитать конструктивно.
%{\color{blue} тут надо дописать n простых строчек по обращению матрицы и вычислению $d$. У $d$ получим, что при $x=0$ достигается 2, значит $0$ — точка плана по теореме эквивалентности. Вторую точку можно будет найти, продифференцировав определитель. }
\end{proof}

\begin{thm}
При $k=2$ существует единственный $D$-оптимальный план. Кроме того, этот план будет насыщенным.
\end{thm}


\begin{proof}
{\color{blue} TODO: Тут еще больше вычислений и начинают использоваться системы Чебышева}
\end{proof}
