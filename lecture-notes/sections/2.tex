\section{Постановка задачи оптимального планирования для нелинейных моделей. Теорема эквивалентности для локально оптимальных планов.}


Пусть $N \in \mathbb{N}$, $y_1, …, y_N \in \R$, $x_1, …, x_N \in \mathbb{X}$, где $\mathbb{X}$ некоторое множество, обычно $\R^k$, а $y_1, …, y_N, x_1, …, x_N$ — наши «наблюдения», которые мы будем называть результатами эксперимента. 

Введем множество параметров $\Theta$ и предположим, что наблюдения описываются следующей моделью:
\begin{equation}
\label{basicModel}
y_i = \eta(x_j, \theta) + \varepsilon_j,
\end{equation}
где $\theta \in \Theta$ — параметр, значения которого мы и будем пытаться в дальнейшем оценить, а $\varepsilon_j$ — случайный шум, про который мы предположим, что
$$ E\varepsilon = 0, E\varepsilon^2 = \sigma^2$$

Будем предполагать, что $\Theta \subset \R^m$.

\begin{dfn}
Будем говорить, что параметр $\theta_j$ входит в \eqref{basicModel} нелинейным образом, если для фиксированного $x \in \mathbb{X}$ существует и не является постоянной функция 
$$ \phi_{j,x}(\theta) = \frac{\partial\eta(x,\theta)}{\theta_j} $$
Если $\phi_{j,x}(\theta) = const$, то $\theta_j$ входит в модель линейным образом.
\end{dfn}

\begin{dfn}
Под точным планом эксперимента будем понимать $N$ точек $x_1, …, x_N \in \mathbb{X}$
\end{dfn}
\begin{dfn}
Под приближенным планом эксперимента будем понимать $n \in \N$ пар $(x_i, \mu_i)$, где 
$$x_i \in \mathbb{X}, x_i \neq x_j, i \neq j,$$ 
$$\mu_i > 0, \sum\limits_{i=1}^{n} \mu_i = 1,$$
\end{dfn}

Пусть $N$ — доступное число «ресурсов» (кол-во экспериментов, которое можно провести). Тогда при использование приближенного плана рекомендуется в точке $x_j$ провести $\mu_jN$ экспериментов. В итоге получится точный план, как работать с которым уже ясно.

\begin{dfn}
При фиксированном плане для оценки $\theta$ будем использовать метод наименьших квадратов:
$$ \hat{\theta} = \argmin\limits_{\theta \in \Theta} \sum \limits_{j=1}^{N} \left(\eta(x_j, \theta) - y_j)^2 \right)$$
\end{dfn}


Наша задача — выбрать некоторым образом точки $x_1, …, x_N$, чтобы МНК-оценка была в некотором смысле оптимальной. 

Введем еще несколько обозначений:
\begin{dfn}
Пусть $\xi$ — дискретная вероятностная мера c носителем $x_1,…, x_n$. Тогда
$$ \int\limits_{\mathbb{X}} g(x) d\xi(x) = \sum \limits_{i=1}^{n}  g(x_i)\xi_i$$
\end{dfn}

\begin{dfn}
Пусть $\Tr{f(x,\theta)} = \left(\frac{\partial\eta(x,\theta)}{\partial \theta_1}, …, \frac{\partial\eta(x,\theta)}{\partial \theta_1}\right)$. 
Пусть $\theta^u$ — истинной значение оцениваемого параметра. Тогда информационной матрицей будем называть
$$M(\xi, \theta_u) = \int\limits_{\mathbb{X}} f(x, \theta)\Tr{f(x, \theta)}d\xi(x)$$
\end{dfn}


Заметим, что $M(\xi, \theta_u)$ в случае, когда все параметры входят линейно, не зависит от $\theta_u$ и  т.к. обратная к информационной матрице — «нижняя оценка» на дисперсию оцениваемого параметра (в многомерном случае под дисперсией  понимается ковариационная матрица), то можно естественным образом ввести различные понятия оптимальности, опираясь на собственные числа информационной матрицы. Например, D-критерий предлагает выбирать планы, максимизирующие определитель информационной матрицы. 

В нелинейном случае все сложнее. Информационная матрица зависит от «истинного» значения параметра, которое неизвестно. Предположим, что у нас есть некоторое приближение $\theta^0$ «истинного» параметра. Тогда будем называть план, максимизирующий определитель матрицы $M(\xi, \theta^0)$ локально D-оптимальным. 

Разложим $\eta$ в ряд Тейлора в окресности $\theta^0 \in \Theta \subset \R^m$\footnote{Фактически мы используем формулу конечных приращений, для которой нужна, конечно, дифференцируемость функции.
В принципе, мы можем предполагать, что все требования асимптотической нормальности выполнены, тогда функция будет дифференцированной. В остальных случаях, это нужно обговаривать отдельно.}:
$$\eta(x, \theta) = \eta(x, \theta^0) + \Tr{(\theta-\theta^0)}f(x, \theta^0) + r(x,\theta)$$
Введем следующие обозначения:
$$ \Tr{f(x)} = \Tr{f(x, \theta^0)} = \left(\frac{\partial \eta(x, \theta^0)}{\partial\theta_1}, …, \frac{\partial \eta(x, \theta^0)}{\partial \theta_m}\right)$$
$$ M(\xi) = M(\xi, \theta^0) = \int\limits_{\mathbb{X}} f(x) \Tr{f(x)}d\xi(x)$$
$$d(x, \xi) = \Tr{f(x)} M^{-1}(\xi)f(x)$$

Для данных обозначение будет верна следующая теорема:
\begin{thm}[Эквивалентности]
План $\xi^*$ является локально D-оптимальным для модели \eqref{basicModel} тогда и только тогда, когда 
$$ m = \max\limits_{x\in \mathbb{X}} d(x, \xi^*),$$
где $m$ --- размерность вектора параметров.
Кроме того, 
$$ \max \limits_{x \in \mathbb{X}} d(x, \xi^*) = \inf\limits_{\xi}\max\limits_{x\in\mathbb{X}} d(x, \xi)$$
Функция $d(x, \xi^*)$ достигает максимального значения во всех точках любого локального D-оптимального плана.
Информационные матрицы всех локально D-оптимального планов совпадают.
\end{thm}
\begin{proof}
Без доказательства. Является переформулировкой теоремы Кифера-Вольфовица (которая видимо была раньше).
\end{proof}



